{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"#–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –º–æ–¥–µ–ª–∏","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_state_split = 42\nDropout_num = 0\nlearning_rate = 6e-6\nvalid = 0.2\nepochs_num = 3\nbatch_size_num = 16\ntarget_corrected = False\ntarget_big_corrected = False","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"–ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫ (–Ω–µ–º–Ω–æ–≥–æ –ª–∏—à–Ω–∏—Ö –µ—Å—Ç—å))"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\n\nimport seaborn as sns\n\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\n\nfrom wordcloud import WordCloud\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.metrics import classification_report,confusion_matrix\n\nfrom collections import defaultdict\nfrom collections import Counter\n\nstop=set(stopwords.words('english'))\n\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\n\nfrom tqdm import tqdm\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM,Dense, SpatialDropout1D, Dropout\nfrom keras.initializers import Constant\nfrom keras.optimizers import Adam","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## —Å–∫–∞—á–∏–≤–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç"},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet= pd.read_csv('../input/nlp-getting-started/train.csv')\ntest=pd.read_csv('../input/nlp-getting-started/test.csv')\nsubmission = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('There are {} rows and {} columns in train'.format(tweet.shape[0],tweet.shape[1]))\nprint('There are {} rows and {} columns in train'.format(test.shape[0],test.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Cleaning <a class=\"anchor\" id=\"5\"></a>\n\n–í –ø—Ä–∏–Ω—Ü–∏–ø–µ, –∑–¥–µ—Å—å –Ω–∏—á–µ–≥–æ –Ω–æ–≤–æ–≥–æ –Ω–µ—Ç, —Ç–æ —á—Ç–æ –¥–µ–ª–∞–µ—Ç—Å—è –≤ 90% —Ä–∞–±–æ—Ç"},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.concat([tweet,test])\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### urls"},{"metadata":{"trusted":true},"cell_type":"code","source":"example=\"New competition launched :https://www.kaggle.com/c/nlp-getting-started\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_URL(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\nremove_URL(example)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text']=df['text'].apply(lambda x : remove_URL(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### HTML tags"},{"metadata":{"trusted":true},"cell_type":"code","source":"example = \"\"\"<div>\n<h1>Real or Fake</h1>\n<p>Kaggle </p>\n<a href=\"https://www.kaggle.com/c/nlp-getting-started\">getting started</a>\n</div>\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\nprint(remove_html(example))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text']=df['text'].apply(lambda x : remove_html(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Emojis + numbers"},{"metadata":{"trusted":true},"cell_type":"code","source":"# –î–æ–±–∞–≤–∏–ª —Ç—É—Ç –∂–µ —É–¥–∞–ª–µ–Ω–∏–µ —Ü–∏—Ñ—Ä\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+|(\\s\\d+)\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\nremove_emoji(\"Omg another Earthquake üòîüòî 2\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text']=df['text'].apply(lambda x: remove_emoji(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### punctuations"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\nexample=\"I am a #king\"\nprint(remove_punct(example))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text']=df['text'].apply(lambda x : remove_punct(x))\nprint(df['text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#–î–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ –º–æ–º–µ–Ω—Ç–∞ —É–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤ –∏–∑ —Ç–≤–∏—Ç–∞ –¥–∞–≤–∞–ª–∞ –≤–Ω—É—à–∏—Ç–µ–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç\ndef remove_stop(text):\n    return [word for word in df['text'] if word not in stop]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df['text']= remove_stop(df['text'])\n# print(df['text'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## BERT –∏ TFHub <a class=\"anchor\" id=\"10\"></a>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# –°–∫—Ä–∏–ø—Ç –æ—Ç –≥—É–≥–ª–∞\n!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#–ï—â—ë –Ω–µ–º–Ω–æ–≥–æ –±–∏–±–ª–∏–æ—Ç–µ–∫\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\n\nimport tokenization","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# –ü—Ä–æ–±–æ–≤–∞–ª –¥—Ä—É–≥–∏–µ –Ω–µ–π—Ä–æ–Ω–∫–∏, —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–ª—É—á–∞–ª–∏—Å—å —Ö—É–∂–µ\ndef bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# –°—Ç—Ä–æ–∏–º –º–æ–¥–µ–ª—å\ndef build_model(bert_layer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    \n    if Dropout_num == 0:\n        # Without Dropout\n        out = Dense(1, activation='sigmoid')(clf_output)\n    else:\n        # With Dropout(Dropout_num), Dropout_num > 0\n        x = Dropout(Dropout_num)(clf_output)\n        out = Dense(1, activation='sigmoid')(x)\n\n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# –ó–∞–º–µ–Ω–∞ –∞–±–±—Ä–µ–∞–∏–≤–∞—Ç—É—Ä –∏ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π\nabbreviations = {\n    \"$\" : \" dollar \",\n    \"‚Ç¨\" : \" euro \",\n    \"4ao\" : \"for adults only\",\n    \"a.m\" : \"before midday\",\n    \"a3\" : \"anytime anywhere anyplace\",\n    \"aamof\" : \"as a matter of fact\",\n    \"acct\" : \"account\",\n    \"adih\" : \"another day in hell\",\n    \"afaic\" : \"as far as i am concerned\",\n    \"afaict\" : \"as far as i can tell\",\n    \"afaik\" : \"as far as i know\",\n    \"afair\" : \"as far as i remember\",\n    \"afk\" : \"away from keyboard\",\n    \"app\" : \"application\",\n    \"approx\" : \"approximately\",\n    \"apps\" : \"applications\",\n    \"asap\" : \"as soon as possible\",\n    \"asl\" : \"age, sex, location\",\n    \"atk\" : \"at the keyboard\",\n    \"ave.\" : \"avenue\",\n    \"aymm\" : \"are you my mother\",\n    \"ayor\" : \"at your own risk\", \n    \"b&b\" : \"bed and breakfast\",\n    \"b+b\" : \"bed and breakfast\",\n    \"b.c\" : \"before christ\",\n    \"b2b\" : \"business to business\",\n    \"b2c\" : \"business to customer\",\n    \"b4\" : \"before\",\n    \"b4n\" : \"bye for now\",\n    \"b@u\" : \"back at you\",\n    \"bae\" : \"before anyone else\",\n    \"bak\" : \"back at keyboard\",\n    \"bbbg\" : \"bye bye be good\",\n    \"bbc\" : \"british broadcasting corporation\",\n    \"bbias\" : \"be back in a second\",\n    \"bbl\" : \"be back later\",\n    \"bbs\" : \"be back soon\",\n    \"be4\" : \"before\",\n    \"bfn\" : \"bye for now\",\n    \"blvd\" : \"boulevard\",\n    \"bout\" : \"about\",\n    \"brb\" : \"be right back\",\n    \"bros\" : \"brothers\",\n    \"brt\" : \"be right there\",\n    \"bsaaw\" : \"big smile and a wink\",\n    \"btw\" : \"by the way\",\n    \"bwl\" : \"bursting with laughter\",\n    \"c/o\" : \"care of\",\n    \"cet\" : \"central european time\",\n    \"cf\" : \"compare\",\n    \"cia\" : \"central intelligence agency\",\n    \"csl\" : \"can not stop laughing\",\n    \"cu\" : \"see you\",\n    \"cul8r\" : \"see you later\",\n    \"cv\" : \"curriculum vitae\",\n    \"cwot\" : \"complete waste of time\",\n    \"cya\" : \"see you\",\n    \"cyt\" : \"see you tomorrow\",\n    \"dae\" : \"does anyone else\",\n    \"dbmib\" : \"do not bother me i am busy\",\n    \"diy\" : \"do it yourself\",\n    \"dm\" : \"direct message\",\n    \"dwh\" : \"during work hours\",\n    \"e123\" : \"easy as one two three\",\n    \"eet\" : \"eastern european time\",\n    \"eg\" : \"example\",\n    \"embm\" : \"early morning business meeting\",\n    \"encl\" : \"enclosed\",\n    \"encl.\" : \"enclosed\",\n    \"etc\" : \"and so on\",\n    \"faq\" : \"frequently asked questions\",\n    \"fawc\" : \"for anyone who cares\",\n    \"fb\" : \"facebook\",\n    \"fc\" : \"fingers crossed\",\n    \"fig\" : \"figure\",\n    \"fimh\" : \"forever in my heart\", \n    \"ft.\" : \"feet\",\n    \"ft\" : \"featuring\",\n    \"ftl\" : \"for the loss\",\n    \"ftw\" : \"for the win\",\n    \"fwiw\" : \"for what it is worth\",\n    \"fyi\" : \"for your information\",\n    \"g9\" : \"genius\",\n    \"gahoy\" : \"get a hold of yourself\",\n    \"gal\" : \"get a life\",\n    \"gcse\" : \"general certificate of secondary education\",\n    \"gfn\" : \"gone for now\",\n    \"gg\" : \"good game\",\n    \"gl\" : \"good luck\",\n    \"glhf\" : \"good luck have fun\",\n    \"gmt\" : \"greenwich mean time\",\n    \"gmta\" : \"great minds think alike\",\n    \"gn\" : \"good night\",\n    \"g.o.a.t\" : \"greatest of all time\",\n    \"goat\" : \"greatest of all time\",\n    \"goi\" : \"get over it\",\n    \"gps\" : \"global positioning system\",\n    \"gr8\" : \"great\",\n    \"gratz\" : \"congratulations\",\n    \"gyal\" : \"girl\",\n    \"h&c\" : \"hot and cold\",\n    \"hp\" : \"horsepower\",\n    \"hr\" : \"hour\",\n    \"hrh\" : \"his royal highness\",\n    \"ht\" : \"height\",\n    \"ibrb\" : \"i will be right back\",\n    \"ic\" : \"i see\",\n    \"icq\" : \"i seek you\",\n    \"icymi\" : \"in case you missed it\",\n    \"idc\" : \"i do not care\",\n    \"idgadf\" : \"i do not give a damn fuck\",\n    \"idgaf\" : \"i do not give a fuck\",\n    \"idk\" : \"i do not know\",\n    \"ie\" : \"that is\",\n    \"i.e\" : \"that is\",\n    \"ifyp\" : \"i feel your pain\",\n    \"IG\" : \"instagram\",\n    \"iirc\" : \"if i remember correctly\",\n    \"ilu\" : \"i love you\",\n    \"ily\" : \"i love you\",\n    \"imho\" : \"in my humble opinion\",\n    \"imo\" : \"in my opinion\",\n    \"imu\" : \"i miss you\",\n    \"iow\" : \"in other words\",\n    \"irl\" : \"in real life\",\n    \"j4f\" : \"just for fun\",\n    \"jic\" : \"just in case\",\n    \"jk\" : \"just kidding\",\n    \"jsyk\" : \"just so you know\",\n    \"l8r\" : \"later\",\n    \"lb\" : \"pound\",\n    \"lbs\" : \"pounds\",\n    \"ldr\" : \"long distance relationship\",\n    \"lmao\" : \"laugh my ass off\",\n    \"lmfao\" : \"laugh my fucking ass off\",\n    \"lol\" : \"laughing out loud\",\n    \"ltd\" : \"limited\",\n    \"ltns\" : \"long time no see\",\n    \"m8\" : \"mate\",\n    \"mf\" : \"motherfucker\",\n    \"mfs\" : \"motherfuckers\",\n    \"mfw\" : \"my face when\",\n    \"mofo\" : \"motherfucker\",\n    \"mph\" : \"miles per hour\",\n    \"mr\" : \"mister\",\n    \"mrw\" : \"my reaction when\",\n    \"ms\" : \"miss\",\n    \"mte\" : \"my thoughts exactly\",\n    \"nagi\" : \"not a good idea\",\n    \"nbc\" : \"national broadcasting company\",\n    \"nbd\" : \"not big deal\",\n    \"nfs\" : \"not for sale\",\n    \"ngl\" : \"not going to lie\",\n    \"nhs\" : \"national health service\",\n    \"nrn\" : \"no reply necessary\",\n    \"nsfl\" : \"not safe for life\",\n    \"nsfw\" : \"not safe for work\",\n    \"nth\" : \"nice to have\",\n    \"nvr\" : \"never\",\n    \"nyc\" : \"new york city\",\n    \"oc\" : \"original content\",\n    \"og\" : \"original\",\n    \"ohp\" : \"overhead projector\",\n    \"oic\" : \"oh i see\",\n    \"omdb\" : \"over my dead body\",\n    \"omg\" : \"oh my god\",\n    \"omw\" : \"on my way\",\n    \"p.a\" : \"per annum\",\n    \"p.m\" : \"after midday\",\n    \"pm\" : \"prime minister\",\n    \"poc\" : \"people of color\",\n    \"pov\" : \"point of view\",\n    \"pp\" : \"pages\",\n    \"ppl\" : \"people\",\n    \"prw\" : \"parents are watching\",\n    \"ps\" : \"postscript\",\n    \"pt\" : \"point\",\n    \"ptb\" : \"please text back\",\n    \"pto\" : \"please turn over\",\n    \"qpsa\" : \"what happens\", #\"que pasa\",\n    \"ratchet\" : \"rude\",\n    \"rbtl\" : \"read between the lines\",\n    \"rlrt\" : \"real life retweet\", \n    \"rofl\" : \"rolling on the floor laughing\",\n    \"roflol\" : \"rolling on the floor laughing out loud\",\n    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n    \"rt\" : \"retweet\",\n    \"ruok\" : \"are you ok\",\n    \"sfw\" : \"safe for work\",\n    \"sk8\" : \"skate\",\n    \"smh\" : \"shake my head\",\n    \"sq\" : \"square\",\n    \"srsly\" : \"seriously\", \n    \"ssdd\" : \"same stuff different day\",\n    \"tbh\" : \"to be honest\",\n    \"tbs\" : \"tablespooful\",\n    \"tbsp\" : \"tablespooful\",\n    \"tfw\" : \"that feeling when\",\n    \"thks\" : \"thank you\",\n    \"tho\" : \"though\",\n    \"thx\" : \"thank you\",\n    \"tia\" : \"thanks in advance\",\n    \"til\" : \"today i learned\",\n    \"tl;dr\" : \"too long i did not read\",\n    \"tldr\" : \"too long i did not read\",\n    \"tmb\" : \"tweet me back\",\n    \"tntl\" : \"trying not to laugh\",\n    \"ttyl\" : \"talk to you later\",\n    \"u\" : \"you\",\n    \"u2\" : \"you too\",\n    \"u4e\" : \"yours for ever\",\n    \"utc\" : \"coordinated universal time\",\n    \"w/\" : \"with\",\n    \"w/o\" : \"without\",\n    \"w8\" : \"wait\",\n    \"wassup\" : \"what is up\",\n    \"wb\" : \"welcome back\",\n    \"wtf\" : \"what the fuck\",\n    \"wtg\" : \"way to go\",\n    \"wtpa\" : \"where the party at\",\n    \"wuf\" : \"where are you from\",\n    \"wuzup\" : \"what is up\",\n    \"wywh\" : \"wish you were here\",\n    \"yd\" : \"yard\",\n    \"ygtr\" : \"you got that right\",\n    \"ynk\" : \"you never know\",\n    \"zzz\" : \"sleeping bored and tired\",\n#     \"he's\" : \"he is\",\n#     \"there's\" : \"there is\",\n#     \"We're\" : \"We are\",\n#     \"That's\" : \"That is\",\n#     \"won't\" : \"will not\",\n#     \"they're\" : \"they are\",\n#     \"Can't\" : \"Cannot\",\n#     \"wasn't\" : \"was not\",\n#     \"don\\x89√õ¬™t\" : \"do not\",\n#     \"aren't\" : \"are not\",\n#     \"isn't\" : \"is not\",\n#     \"What's\" : \"What is\",\n#     \"haven't\" : \"have not\",\n#     \"hasn't\" : \"has not\",\n#     \"There's\" : \"There is\",\n#     \"He's\" : \"He is\",\n#     \"It's\" : \"It is\",\n#     \"You're\" : \"You are\",\n#     \"I'M\" : \"I am\",\n#     \"shouldn't\" : \"should not\",\n#     \"wouldn't\" : \"would not\",\n#     \"i'm\" : \"I am\",\n#     \"I\\x89√õ¬™m\" : \"I am\",\n#     \"I'm\" : \"I am\",\n#     \"Isn't\" : \"is not\",\n#     \"Here's\" : \"Here is\",\n#     \"you've\" : \"you have\",\n#     \"you\\x89√õ¬™ve\" : \"you have\",\n#     \"we're\" : \"we are\",\n#     \"what's\" : \"what is\",\n#     \"couldn't\" : \"could not\",\n#     \"we've\" : \"we have\",\n#     \"it\\x89√õ¬™s\" : \"it is\",\n#     \"doesn\\x89√õ¬™t\" : \"does not\",\n#     \"It\\x89√õ¬™s\" : \"It is\",\n#     \"Here\\x89√õ¬™s\" : \"Here is\",\n#     \"who's\" : \"who is\",\n#     \"I\\x89√õ¬™ve\" : \"I have\",\n#     \"y'all\" : \"you all\",\n#     \"can\\x89√õ¬™t\" : \"cannot\",\n#     \"would've\" : \"would have\",\n#     \"it'll\" : \"it will\",\n#     \"we'll\" : \"we will\",\n#     \"wouldn\\x89√õ¬™t\" : \"would not\",\n#     \"We've\" : \"We have\",\n#     \"he'll\" : \"he will\",\n#     \"Y'all\" : \"You all\",\n#     \"Weren't\" : \"Were not\",\n#     \"Didn't\" : \"Did not\",\n#     \"they'll\" : \"they will\",\n#     \"they'd\" : \"they would\",\n#     \"DON'T\" : \"DO NOT\",\n#     \"That\\x89√õ¬™s\" : \"That is\",\n#     \"they've\" : \"they have\",\n#     \"i'd\" : \"I would\",\n#     \"should've\" : \"should have\",\n#     \"You\\x89√õ¬™re\" : \"You are\",\n#     \"where's\" : \"where is\",\n#     \"Don\\x89√õ¬™t\" : \"Do not\",\n#     \"we'd\" : \"we would\",\n#     \"i'll\" : \"I will\",\n#     \"weren't\" : \"were not\",\n#     \"They're\" : \"They are\",\n#     \"Can\\x89√õ¬™t\" : \"Cannot\",\n#     \"you\\x89√õ¬™ll\" : \"you will\",\n#     \"I\\x89√õ¬™d\" : \"I would\",\n#     \"let's\" : \"let us\",\n#     \"it's\" : \"it is\",\n#     \"can't\" : \"cannot\",\n#     \"don't\" : \"do not\",\n#     \"you're\" : \"you are\",\n#     \"i've\" : \"I have\",\n#     \"that's\" : \"that is\",\n#     \"i'll\" : \"I will\",\n#     \"doesn't\" : \"does not\",\n#     \"i'd\" : \"I would\",\n#     \"didn't\" : \"did not\",\n#     \"ain't\" : \"am not\",\n#     \"you'll\" : \"you will\",\n#     \"I've\" : \"I have\",\n#     \"Don't\" : \"do not\",\n#     \"I'll\" : \"I will\",\n#     \"I'd\" : \"I would\",\n#     \"Let's\" : \"Let us\",\n#     \"you'd\" : \"You would\",\n#     \"It's\" : \"It is\",\n#     \"Ain't\" : \"am not\",\n#     \"Haven't\" : \"Have not\",\n#     \"Could've\" : \"Could have\",\n#     \"youve\" : \"you have\",\n#     \"don√•¬´t\" : \"do not\" –ü–æ-—Ö–æ—Ä–æ—à–µ–º—É, –æ—Ç—Å—é–¥–∞ –Ω—É–∂–Ω–æ –±—ã–ª–æ –∏—Å–∫–ª—é—á–∏—Ç—å —Å—Ç–æ–ø —Å–ª–æ–≤–∞\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_abbrev(word):\n    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_abbrev_in_text(text):\n    tokens = word_tokenize(text)\n    tokens = [convert_abbrev(word) for word in tokens]\n    text = ' '.join(tokens)\n    return text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Download data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# –°–∫–∞—á–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å\nmodule_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train= pd.read_csv('../input/nlp-getting-started/train.csv')\ntest=pd.read_csv('../input/nlp-getting-started/test.csv')\nsubmission = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Target correction"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Thanks to https://www.kaggle.com/wrrosa/keras-bert-using-tfhub-modified-train-data \n# –ù–µ –ø–æ–Ω—è–ª —á—Ç–æ —ç—Ç–æ, –Ω–æ —Ç–∞–∫ —è–≤–Ω–æ –ª—É—á—à–µ)\nif target_corrected:\n    ids_with_target_error = [328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226]\n    train.loc[train['id'].isin(ids_with_target_error),'target'] = 0\n    train[train['id'].isin(ids_with_target_error)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Thanks to https://www.kaggle.com/rftexas/text-only-kfold-bert\nif target_big_corrected:\n    train[\"text\"] = train[\"text\"].apply(lambda x: clean_tweets(x))\n    test[\"text\"] = test[\"text\"].apply(lambda x: clean_tweets(x))\n    \n    train[\"text\"] = train[\"text\"].apply(lambda x: remove_emoji(x))\n    test[\"text\"] = test[\"text\"].apply(lambda x: remove_emoji(x))\n    \n    train[\"text\"] = train[\"text\"].apply(lambda x: remove_punctuations(x))\n    test[\"text\"] = test[\"text\"].apply(lambda x: remove_punctuations(x))\n    \n    train[\"text\"] = train[\"text\"].apply(lambda x: convert_abbrev_in_text(x))\n    test[\"text\"] = test[\"text\"].apply(lambda x: convert_abbrev_in_text(x))\n    \n    train['text'] = remove_stop(train['text'])\n    test['text'] = remove_stop(test['text'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# –°–æ–∑–¥–∞–µ–º –∏ —Ç—Ä–µ–Ω–µ—Ä—É–µ–º –º–æ–¥–µ–ª—å"},{"metadata":{"trusted":true},"cell_type":"code","source":"# –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏–∑ bert_layer\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# –ö–æ–¥–∏—Ä–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –≤ —Ç–æ–∫–µ–Ω—ã (–º–∞—Ä–∫–µ—Ä—ã, –º–∞—Å–∫–∏ –∏ —Ñ–ª–∞–≥–∏ —Å–µ–≥–º–µ–Ω—Ç–æ–≤)\ntrain_input = bert_encode(train.text.values, tokenizer, max_len=160)\ntest_input = bert_encode(test.text.values, tokenizer, max_len=160)\ntrain_labels = train.target.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# –°—Ç—Ä–æ–∏–º –º–æ–¥–µ–ª—å\nmodel_BERT = build_model(bert_layer, max_len=160)\nmodel_BERT.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# –û–±—É—á–∞–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å\ncheckpoint = ModelCheckpoint('model_BERT.h5', monitor='val_loss', save_best_only=True)\n\ntrain_history = model_BERT.fit(\n    train_input, train_labels,\n    validation_split = valid,\n    epochs = epochs_num,\n    callbacks=[checkpoint],\n    batch_size = batch_size_num\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # –ì–æ—Ç–æ–≤–∏–º —Å–∞–±–º–∏—Ç (–¥–µ–ª–∞–µ–º –ø—Ä–µ–¥–∏–∫—Ç—ã)\nmodel_BERT.load_weights('model_BERT.h5')\ntest_pred_BERT = model_BERT.predict(test_input)\ntest_pred_BERT_int = test_pred_BERT.round().astype('int')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 10.1. Submission by BERT<a class=\"anchor\" id=\"10.1\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['target'] = test_pred_BERT_int\nsubmission.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# –§–∞–π–ª —Å–∞–±–º–∏—Ç–∞\nsubmission.to_csv(\"submission.csv\", index=False, header=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[Go to Top](#0)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"}},"nbformat":4,"nbformat_minor":4}