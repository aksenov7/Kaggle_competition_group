{"cells":[{"metadata":{},"cell_type":"markdown","source":"В этой задаче мы прогнозируем продажи товаров в различных магазинах в течение двух 28-дневных периодов времени.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Files\n* calendar.csv - содержит информацию о датах продажи товаров.\n* sales_train_validation.csv - содержит исторические ежедневные данные о единичных продажах для каждого продукта и магазина [d_1 - d_1913]\n* sample_submission.csv - правильный формат для отправки ответов.\n* sell_prices.csv - содержит информацию о цене проданных товаров в каждом магазине и дате их продажи.\n* sales_train_evaluation.csv - доступно за месяц до окончания конкурса. Будет включать продажи [d_1 - d_1941]","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"File 1: “calendar.csv” содержит информацию о датах продажи товаров. \n* date: дата в формате “y-m-d”. \n* wm_yr_wk: идентификатор недели, к которой относится дата. \n* weekday: тип дня (суббота, воскресенье,..., пятница). \n* wday: идентификатор рабочего дня, начиная с субботы. \n* month: месяц даты. \n* year: год даты. \n* event_name_1: если дата включает в себя событие, то имя этого события. \n* event_type_1: если дата включает в себя событие, то тип этого события. \n* event_name_2: если дата включает в себя второе событие, то имя этого события. \n* event_type_2: если дата включает в себя второе событие, то тип этого события. \n* snap_CA, snap_TX, and snap_WI: двоичная переменная (0 или 1), указывающая, разрешают ли магазины CA, TX или WI совершать покупки SNAP на исследуемую дату. 1 указывает, что разрешены срочные покупки.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"File 2: “sell_prices.csv” Содержит информацию о цене продаваемых товаров в каждом магазине и дате их продажи. \n* store_id: идентификатор магазина, в котором продается товар. \n* item_id: идентификатор продукта. \n* wm_yr_wk: идентификатор недели. \n* sell_price: цена товара за данную неделю / магазин. Цена указана за неделю (в среднем за семь дней). Если он отсутствует, это означает, что продукт не был продан в течение исследуемой недели. Обратите внимание, что хотя цены являются постоянными на еженедельной основе, они могут меняться с течением времени (как обучение, так и набор тестов).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"File 3: “sales_train.csv” содержит исторические ежедневные данные о единичных продажах для каждого продукта и магазина. \n* item_id: идентификатор продукта. \n* dept_id: идентификатор отдела, к которому принадлежит продукт. \n* cat_id: идентификатор категории, к которой относится продукт. \n* store_id: идентификатор магазина, в котором продается товар. \n* state_id: состояние, в котором находится магазин. \n* d_1, d_2, …, d_i, … d_1941: количество единиц, проданных в день i, Начиная с 2011-01-29.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os, sys, gc, time, warnings, pickle, psutil, random\n\nfrom multiprocessing import Pool\n\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Seeder\n# seed to make all processes deterministic\ndef seed_everything(seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n\n    \n## Multiprocess Runs\ndef df_parallelize_run(func, t_split):\n    num_cores = np.min([N_CORES,len(t_split)])\n    pool = Pool(num_cores)\n    df = pd.concat(pool.map(func, t_split), axis=1)\n    pool.close()\n    pool.join()\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Помощник для загрузки данных по идентификатору хранилища**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Чтение данных\n# store - идентификатор магазина\ndef get_data_by_store(store):\n    # Чтение c использованием функции concat\n    df = pd.concat(\n        [\n            pd.read_pickle(BASE),\n            pd.read_pickle(PRICE).iloc[:,2:],\n            pd.read_pickle(CALENDAR).iloc[:,2:]\n        ],\n        axis=1\n    )\n    \n    # Оставляем только соответствующий магазин\n    df = df[df['store_id'] == store]\n\n    # Из-за ограничения памяти мы должны читать лаги и средние характеристики отдельно и отбрасывать элементы, которые нам не нужны.\n    # Поскольку наши сетки объектов выровнены, мы можем использовать индекс, чтобы сохранить только необходимые строки.\n    # Выравнивание хорошо для нас, так как concat использует меньше памяти, чем merge.\n    df2 = pd.read_pickle(MEAN_ENC)[mean_features]\n    df2 = df2[df2.index.isin(df.index)]\n    \n    df3 = pd.read_pickle(LAGS).iloc[:,3:]\n    df3 = df3[df3.index.isin(df.index)]\n    \n    df = pd.concat([df, df2], axis=1)\n    del df2 # чтобы не достичь предела памяти\n    \n    df = pd.concat([df, df3], axis=1)\n    del df3 # чтобы не достичь предела памяти\n    \n    # Создание списка особенностей\n    features = [col for col in list(df) if col not in remove_features]\n    df = df[['id','d',TARGET] + features]\n    \n    # Пропуск первых n строк\n    df = df[df['d'] >= START_TRAIN].reset_index(drop=True)\n    \n    return df, features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Рекомбинированный тестовый набор после обучения\ndef get_base_test():\n    base_test = pd.DataFrame()\n\n    for store_id in STORES_IDS:\n        temp_df = pd.read_pickle('test_' + store_id + '.pkl')\n        temp_df['store_id'] = store_id\n        base_test = pd.concat([base_test, temp_df]).reset_index(drop=True)\n    \n    return base_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Создание динамических лагов**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_lag(LAG_DAY):\n    lag_df = base_test[['id', 'd', TARGET]]\n    col_name = 'sales_lag_' + str(LAG_DAY)\n    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(LAG_DAY)).astype(np.float16)\n    return lag_df[[col_name]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_lag_roll(LAG_DAY):\n    shift_day = LAG_DAY[0]\n    roll_wind = LAG_DAY[1]\n    lag_df = base_test[['id', 'd', TARGET]]\n    col_name = 'rolling_mean_tmp_' + str(shift_day) + '_' + str(roll_wind)\n    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(shift_day).rolling(roll_wind).mean())\n    return lag_df[[col_name]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Параметры модели**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nlgb_params = {\n    'boosting_type': 'gbdt',\n    'objective': 'tweedie',\n    'tweedie_variance_power': 1.1,\n    'metric': 'rmse',\n    'subsample': 0.5,\n    'subsample_freq': 1,\n    'learning_rate': 0.03,\n    'num_leaves': 2**11-1,\n    'min_data_in_leaf': 2**12-1,\n    'feature_fraction': 0.5,\n    'max_bin': 100,\n    'n_estimators': 1400,\n    'boost_from_average': False,\n    'verbose': -1,\n} ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**boosting_type**\n* 'gbdt' - стандартный метод для повышения градиента\n* 'goss' - для более быстрого обучения, но обычно это приводит к недостаточной приспособленности\n* 'dart' - для очень долгого обучения, а производительность модели зависит от случайного фактора\n\n**objective**\n\nПовышение градиента tweediie для чрезвычайно несбалансированных нулевых завышенных данных.\n\n**tweedie_variance_power**\n\n* default = 1.5\n* Установите это значение ближе к 2, чтобы перейти к гамма распределению\n* Установите это значение ближе к 1, чтобы перейти к распределению Пуассона\n* Значение 1.1 является самым оптимальным\n\n**metric**\n\nЭто ничего не значит для нас, так как метрика конкуренции отличается, и мы не используем здесь ранние остановки.\nТаким образом, rmse служит только для общего обзора производительности модели.\nКроме того, мы используем \"поддельный\" набор валидации (поскольку он входит в состав тренировочного набора),\nпоэтому даже общий балл rmse ничего не значит.\n\n**subsample**\n\nСлужит для борьбы с overfit\nбудет случайным образом выбирать часть данных без передискретизации\nБыл выбран CV\n\n**subsample_freq**\n\nчастота для упаковки\ndefault value кажется в порядке\n\n**learning_rate**\n\nВыбирается по CV\nSmaller - дольше тренировка, но есть возможность остановиться в \"локальном минимуме\"\nBigger - быстрее тренировка, но есть шанс не найти \"глобальный минимум\" минимума\n\n**num_leaves и min_data_in_leaf**\n\nПринудительная модель для использования дополнительных функций\nНам это нужно, чтобы уменьшить влияние \"рекурсивных\" ошибок\nКроме того, это приводит к переоснащению, поэтому мы используем маленький 'max_bin': 100\n\n**l1, l2 регуляризации**\n\nl2 может работать с большими num_leaves\n                    \n**n_estimators**\n\nCV показывает, что для каждого состояния/хранилища должны быть разные значения.\nТекущее значение было выбрано для общего назначения.\nПоскольку мы не используем никаких ранних остановок, осторожно, чтобы не перегружать Public LB.\n\n**feature_fraction**\n\nLightGBM будет случайным образом выбирать часть объектов на каждой итерации (tree).\nУ нас есть много функций, и многие из них являются \"дубликатами\", а многие просто \"шумят\" хорошие значения здесь - 0.5-0.7 (by CV)\n\n**boost_from_average**\n\nСуществует некоторая \"проблема\" в коде boost_from_average для пользовательской потери\n'True' делает обучение более быстрым, но тщательно используйте его","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Переменные**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"VER = 1                          # Версия нашей модели\nN_CORES = psutil.cpu_count()     # Количество доступных ядер процессора\n\n# Мы хотим, чтобы всё было как можно более детерминированным\nSEED = 42\nseed_everything(SEED)\nlgb_params['seed'] = SEED\n\n# Лимиты и константы\nTARGET      = 'sales'            # Наша цель\nSTART_TRAIN = 0                  # Мы можем пропустить несколько строк (Nans/faster training)\nEND_TRAIN   = 1913               # Последний день для тренировки\nP_HORIZON   = 28                 # Количество дней, на которые создается предсказание\nUSE_AUX     = True               # Использование предварительного обучения модели\n\n# Удаляемые характеристики\nremove_features = [\n    'id',\n    'state_id',\n    'store_id',\n    'date',\n    'wm_yr_wk',\n    'd',\n    TARGET\n]\n\n# Средние характеристики\nmean_features = [\n    'enc_cat_id_mean',\n    'enc_cat_id_std',\n    'enc_dept_id_mean',\n    'enc_dept_id_std',\n    'enc_item_id_mean',\n    'enc_item_id_std'\n] \n\n# Пути\nORIGINAL = '../input/m5-forecasting-accuracy/'\nBASE     = '../input/m5-simple-fe/grid_part_1.pkl'\nPRICE    = '../input/m5-simple-fe/grid_part_2.pkl'\nCALENDAR = '../input/m5-simple-fe/grid_part_3.pkl'\nLAGS     = '../input/m5-lags-features/lags_df_28.pkl'\nMEAN_ENC = '../input/m5-custom-features/mean_encoding_df.pkl'\n\n# AUX (предварительное обучение) Путь до модели\nAUX_MODELS = '../input/m5-aux-models/'\n\n# Идентификаторы магазинов\nSTORES_IDS = pd.read_csv(ORIGINAL + 'sales_train_validation.csv')['store_id']\nSTORES_IDS = list(STORES_IDS.unique())\n\n# SPLIT для создания лагов\nSHIFT_DAY  = 28\nN_LAGS     = 15\nLAGS_SPLIT = [col for col in range(SHIFT_DAY,SHIFT_DAY + N_LAGS)]\nROLS_SPLIT = []\nfor i in [1,7,14]:\n    for j in [7,14,30,60]:\n        ROLS_SPLIT.append([i, j])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Aux Models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Если вы не хотите ждать часами, чтобы получить результат, \n# вы можете обучить каждое хранилище в отдельном ядре, а затем просто присоединиться к результату.\n\n# Если мы хотим использовать предварительно обученные модели, мы можем пропустить обучение\nif USE_AUX:\n    lgb_params['n_estimators'] = 2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Модель обучения","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for store_id in STORES_IDS:\n    print('Train', store_id)\n    \n    # Получить grid для текущего магазина\n    grid_df, features_columns = get_data_by_store(store_id)\n    \n    # Маска для обучения (все данные меньше 1913 года)\n    # Валидация (последние 28 дней - не настоящий набор валидаций)\n    # Тест (все данные больше 1913 дня, с некоторым пробелом для рекурсивных функций)\n    train_mask = grid_df['d'] <= END_TRAIN\n    valid_mask = train_mask & (grid_df['d'] > (END_TRAIN - P_HORIZON))\n    preds_mask = grid_df['d'] > (END_TRAIN - 100)\n    \n    # Примените маски и сохраните набор данных lgb в качестве bin, чтобы уменьшить всплески памяти во время преобразования dtype\n    # Чтобы избежать каких-либо конверсий, вы всегда должны использовать np.float32\n    # или сохранить в bin перед началом обучения\n    train_data = lgb.Dataset(grid_df[train_mask][features_columns], label=grid_df[train_mask][TARGET])\n    train_data.save_binary('train_data.bin')\n    train_data = lgb.Dataset('train_data.bin')\n    \n    valid_data = lgb.Dataset(grid_df[valid_mask][features_columns], label=grid_df[valid_mask][TARGET])\n    \n    # Сохранение части набора данных для последующих прогнозов\n    # Удаление объектов, которые мы должны вычислять рекурсивно\n    grid_df = grid_df[preds_mask].reset_index(drop=True)\n    keep_cols = [col for col in list(grid_df) if '_tmp_' not in col]\n    grid_df = grid_df[keep_cols]\n    grid_df.to_pickle('test_'+store_id+'.pkl')\n    del grid_df\n    \n    # Запуск seeder снова, чтобы сделать обучение 100% детерминированным\n    seed_everything(SEED)\n    estimator = lgb.train(\n        lgb_params,\n        train_data,\n        valid_sets = [valid_data],\n        verbose_eval = 100\n    )\n    \n    # Сохранение модели\n    # estimator = lgb.Booster(model_file='model.txt')\n    # можно прогнозировать только с лучшей итерацией (или с сохранением итерации)\n    # pickle.dump дает нам больше гибкости\n    # например estimator.predict(TEST, num_iteration=100)\n    # num_iteration - количество итераций, которые вы хотите предсказать \n    # NULL или <= 0 означает использование наилучшей итерации\n    model_name = 'lgb_model_'+store_id+'_v'+str(VER)+'.bin'\n    pickle.dump(estimator, open(model_name, 'wb'))\n\n    # Удаление временных файлов и объектов, чтобы освободить немного места на жестком диске и оперативной памяти\n    !rm train_data.bin\n    del train_data, valid_data, estimator\n    gc.collect()\n    \n    # Сохранить функции моделей для предсказаний\n    MODEL_FEATURES = features_columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Прогнозирование**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Создание фиктивного фрейма данных для хранения прогнозов\nall_preds = pd.DataFrame()\n\n# Соединие тестового набора данных с небольшой частью обучающих данных, чтобы создать рекурсивные объекты.\nbase_test = get_base_test()\n\n# Таймер для измерения времени предсказаний\nmain_time = time.time()\n\n# Цикл предсказания для каждого дня.\n# Поскольку скользящие лаги являются наиболее трудоёмкими, мы рассчитываем его на целый день.\nfor PREDICT_DAY in range(1, 29):    \n    print('Predict | Day:', PREDICT_DAY)\n    start_time = time.time()\n\n    # Сделайте временную сетку для расчета задержек качения\n    grid_df = base_test.copy()\n    grid_df = pd.concat([grid_df, df_parallelize_run(make_lag_roll, ROLS_SPLIT)], axis=1)\n        \n    for store_id in STORES_IDS:\n        # Считываем все наши модели и делаем прогнозы\n        model_path = 'lgb_model_' + store_id + '_v' + str(VER) + '.bin' \n        if USE_AUX:\n            model_path = AUX_MODELS + model_path\n        \n        estimator = pickle.load(open(model_path, 'rb'))\n        \n        day_mask = base_test['d']==(END_TRAIN+PREDICT_DAY)\n        store_mask = base_test['store_id']==store_id\n        \n        mask = (day_mask)&(store_mask)\n        base_test[TARGET][mask] = estimator.predict(grid_df[mask][MODEL_FEATURES],num_iteration=1400)\n    \n    # Создаем столбцы и добавляем в DataFrame all_preds\n    temp_df = base_test[day_mask][['id',TARGET]]\n    temp_df.columns = ['id','F'+str(PREDICT_DAY)]\n    if 'id' in list(all_preds):\n        all_preds = all_preds.merge(temp_df, on=['id'], how='left')\n    else:\n        all_preds = temp_df.copy()\n        \n    print('#'*10, ' %0.2f min round |' % ((time.time() - start_time) / 60),\n                  ' %0.2f min total |' % ((time.time() - main_time) / 60),\n                  ' %0.2f day sales |' % (temp_df['F'+str(PREDICT_DAY)].sum()))\n    del temp_df\n    \nall_preds = all_preds.reset_index(drop=True)\nall_preds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Экспорт**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Чтение sample_submission и слияние наших прогнозов.\n# Поскольку у нас есть прогнозы только для данных \"_validation\", нам нужно сделать fillna() для элементов \"_evaluation\".\nsubmission = pd.read_csv(ORIGINAL + 'sample_submission.csv')[['id']]\nsubmission = submission.merge(all_preds, on=['id'], how='left').fillna(0)\nsubmission.to_csv('submission_v' + str(VER) + '.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"|num_iteration|LB score|\n|---|---|\n|1400|0.47506|\n|1300|0.47450|\n|1200|0.47505|\n|1100|0.47509|","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}