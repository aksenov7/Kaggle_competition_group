# NLP
  Тут хранятся алгоритмы для обработки текста и решения задач обработки естественных языков. Алгоритм должен принимать на вход текст и выполнять всю предварительную обработку и дальнейшее построение моделей:
  - текст необходимо очистить (предусмотреть какой-то базовый приемлемый вариант очистки, но и оставить возможность прокидывать в функцию свои регулярные выражения для очистки текста)
  - произвести токенизацию (предусмотреть все варианты токенизации, которые можно сделать руками, то есть просто сплитом по пробелам и более тонкие варианты с помощью стандартных токенизаторов библиотеки NLTK)
  - предусмотреть построение словаря как по отдельным словам, так и по n-граммам (когда алгоритм проходит по тексту окном в n слов и они так же помещаются в словарь)
  - предусмотреть построение всех возможных векторных представлений слов (мешок слов, tf-idf, тематические модели (lda, lsi), word2vec, fasttext, doc2vec)
  - предусмотреть применение различных алгоритмов классификации текстов (от стандартных моделей sklearn, до нейронок: rnn, ltsm, gru, cnn  и предварительно обученные модели: ElMO, Transformer, BERT)
  - в случае с BERT присутствует в общем-то практически полный или полный собственный функционал всей предварительной обработки, поэтому возможно ее можно вынести отдельно
