# NLP (постановка задачи)
  Тут хранятся алгоритмы для обработки текста и решения задач обработки естественных языков. Алгоритм должен принимать на вход текст и выполнять всю предварительную обработку и дальнейшее построение моделей:
  - текст необходимо очистить (предусмотреть какой-то базовый приемлемый вариант очистки, но и оставить возможность прокидывать в функцию свои регулярные выражения для очистки текста)
  - произвести токенизацию (предусмотреть все варианты токенизации, которые можно сделать руками, то есть просто сплитом по пробелам и более тонкие варианты с помощью стандартных токенизаторов библиотеки NLTK)
  - предусмотреть построение словаря как по отдельным словам, так и по n-граммам (когда алгоритм проходит по тексту окном в n слов и они так же помещаются в словарь)
  - предусмотреть построение всех возможных векторных представлений слов (мешок слов, tf-idf, тематические модели (lda, lsi), word2vec, fasttext, doc2vec)
  - предусмотреть применение различных алгоритмов классификации текстов (от стандартных моделей sklearn, до нейронок: rnn, ltsm, gru, cnn  и предварительно обученные модели: ElMO, Transformer, BERT)
  - в случае с BERT присутствует в общем-то практически полный или полный собственный функционал всей предварительной обработки, поэтому возможно ее можно вынести отдельно

# nlp-pipeline-for-dummies (реализация)
За основу для создания пайплайнов для NLP взят модуль [consecution](https://github.com/robdmc/consecution), который
позволяет преобразовывать данные и передавать их из одного обработчика в другой в декларативном стиле, который очень
похож на механизм pipe в Unix-системах.

## Install
Следующая команда создаст виртуальное окружение из системного `python3` и установит все необходимые зависимости:
```bash
make install
```

## Nodes
Абстрактные ноды лежат в `nodes/core.py`:
    - `IterableModifierNode` для обработки массива идентичных данных
    - `OnetoOneModifierNode` для обработки одного экземпляра данных

Ноды, реализующие препроцессинг лежат в `nodes/text_preprocessing.py`:
    - `RegexpNode` базовая нода, применяющая набор регулярок из входного параметра `pattern_to_sub_dict`
    - `RegexpTrashReplaceNode` преднастроенная нода с набором дефолтных регулярок, которая заменяет мусор на пустоту

Ноды, реализующие токенизацию лежат в `nodes/text_tokenizing.py`
    - `SpaceTokenizerNode` простой токенизатор, разбивающий текст по пробелам
    - `NLTKTokenizerNode` базовая нода, предустанавливающая параметры для модуля `nltk`
    - `WordNLTKTokenizerNode` преднастроенная нода, использующая для токенизации `nltk.word_tokenize`
    - `SentenceNLTKTokenizerNode` преднастроенная нода, использующая для токенизации `nltk.sent_tokenize`

Ноды, реализующие векторизацию лежат в `nodes/text_vectorizers.py`:
    - `VectorizingNode` базовый узел, который осуществляет инициализацию векторизатора
    - `SKLearnCountVectorizerNode` преднастроенная нода, использующая для векторизации `sklearn.feature_extraction.text.CountVectorizer`
    - `SKLearnTfidVectorizerNode` преднастроенная нода, использующая для векторизации `sklearn.feature_extraction.text.TfidfVectorizer`


## Examples
Все примеры использования имеющихся нод можно увидеть в `test.py`

## TODO
Дописать про `nlp_tools/models.py.`
